<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
  <meta name="generator" content="Bluefish 2.2.12rc2" />
  <meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
  <link rel="stylesheet" href="jemdoc.css" type="text/css" />
  <title>Younes Ben Mazziane</title>
</head>
<body>
  <div id="layout-content">
    <div id="toptitle">
      <h1>Younes Ben Mazziane</h1>
    </div>

    <table class="imgtable">
  <tr>
    <td>
      <img src="Photos/Younes_picture.jpg" alt="Younes Ben Mazziane" width="250px" />&nbsp;
    </td>
    <td align="left">
      <p><a href="https://lia.univ-avignon.fr/">University of Avignon</a><br />
        Avignon, France.<br />
        Email: younesbenmazziane@gmail.com<br />
        <a href="https://scholar.google.com/citations?user=lGi6uH0AAAAJ&hl=en">Google Scholar</a><br />
        <a href="https://dblp.org/pid/317/4857.html">DBLP</a>
      </p>
    </td>
  </tr>
</table>

    <h1>About Me</h1>
    <p>I am a postdoctoral researcher at the University of Avignon under the supervision of <a href="https://scholar.google.com/citations?user=EYyOnEkAAAAJ&hl=fr&oi=ao">Francesco De Pellegrini </a>  and <a href="https://www-sop.inria.fr/members/Eitan.Altman/">Eitan Altman</a>.
        I obtained my Ph.D. in Computer Science from Université Côte d’Azur and Inria Sophia Antipolis in 2024 under the supervision of <a href="https://www-sop.inria.fr/members/Sara.Alouf/">Sara Alouf</a> and <a href="https://www-sop.inria.fr/members/Giovanni.Neglia/">Giovanni Neglia</a>.
        My research is characterized by the application of probability theory to analyze randomized streaming and caching algorithms.
        Recently, I started working on repeated resource allocation games. More details are available in my <a href="Docs/enCV.pdf" download>CV</a>.</p>

    <h1>Contents</h1>
    <ul>
      <li><a href="#publications-simple">Publications</a></li>
      <li><a href="#research-overview">Research Overview</a></li>
    </ul>

    <!-- SIMPLE PUBLICATIONS SECTION -->
    <h2 id="publications-simple">Publications</h2>

    <h3>Preprints</h3>
    <ul>
      <li><a href="https://arxiv.org/abs/2503.02758">Efficient and Optimal No-Regret Caching under Partial Observation</a><br />
          <strong>Younes Ben Mazziane</strong>, Francescomaria Faticanti, Sara Alouf, Giovanni Neglia<br />
          Submitted to <strong>IEEE Transactions on Networking (Major Revision)</strong></li>
    </ul>

    <h3>Journals</h3>
    <ul>
      <li>[J3] <a href="https://dl.acm.org/doi/10.1145/3727135">Universal and Tight Bounds on Counting Errors of Count-Min Sketch with Conservative Updates</a><br />
          <strong>Younes Ben Mazziane</strong>, Othmane Marfoq<br />
          <strong>Proc. ACM Meas. Anal. Comput. Syst.</strong>, Volume 9, 2025</li>
      <li>[J2] <a href="https://doi.org/10.1016/j.comnet.2024.110206">TTL Model for an LRU-based Similarity Caching Policy</a><br />
          <strong>Younes Ben Mazziane</strong>, Sara Alouf, Giovanni Neglia, Daniel S. Menasche<br />
          <strong>Computer Networks</strong>, Volume 241, 2024</li>
      <li>[J1] <a href="https://doi.org/10.1016/j.comnet.2022.109315">Analyzing Count Min Sketch with Conservative Updates</a><br />
          <strong>Younes Ben Mazziane</strong>, Sara Alouf, Giovanni Neglia<br />
          <strong>Computer Networks</strong>, Volume 217, 2022</li>
    </ul>

    <h3>Conferences</h3>
    <ul>
      <li>[C5] <a href="https://dl.acm.org/doi/10.1145/3726854.3727329">Universal and Tight Bounds on Counting Errors of Count-Min Sketch with Conservative Updates</a><br />
          <strong>Younes Ben Mazziane</strong>, Othmane Marfoq<br />
          <strong>ACM SIGMETRICS 2025</strong></li>
      <li>[C4] <a href="https://ieeexplore.ieee.org/document/???">Learning to Bid in Proportional Allocation Auctions with Budget Constraints</a><br />
          <strong>Younes Ben Mazziane</strong>, Cleque-Marlain Mboulou-Moutoubi, Francesco De Pellegrini, Eitan Altman<br />
          <strong>IEEE WiOpt 2025</strong></li>
      <li>[C3] <a href="https://arxiv.org/abs/2309.02055">No-Regret Caching with Noisy Request Estimates</a><br />
          <strong>Younes Ben Mazziane</strong>, Francescomaria Faticanti, Giovanni Neglia, Sara Alouf<br />
          <strong>IEEE VCC 2023</strong></li>
      <li>[C2] <a href="https://ieeexplore.ieee.org/document/10000890">Computing the Hit Rate of Similarity Caching</a><br />
          <strong>Younes Ben Mazziane</strong>, Sara Alouf, Giovanni Neglia, Daniel S. Menasche<br />
          <strong>IEEE GLOBECOM 2022</strong></li>
      <li>[C1] <a href="https://ieeexplore.ieee.org/document/9798146">A Formal Analysis of the Count-Min Sketch with Conservative Updates</a><br />
          <strong>Younes Ben Mazziane</strong>, Sara Alouf, Giovanni Neglia<br />
          <strong>IEEE INFOCOM Workshops 2022</strong></li>
    </ul>

    <!-- RESEARCH OVERVIEW & TOPIC-GROUPED PUBLICATIONS -->
    <h2 id="research-overview">Research Overview</h2>

    <p><strong>Ph.D. thesis (2020–2024).</strong> My Ph.D. thesis, <em>Probabilistic Analysis for Caching</em>, provides rigorous performance guarantees for probabilistic streaming and caching algorithms. The analysis of these algorithms is complex, as it involves studying analytically intractable stochastic processes. We circumvent this difficulty by introducing novel surrogate processes that capture the real dynamics yet remain tractable. These surrogates are then analyzed with Markov-process techniques, renewal theory, concentration inequalities, or online convex optimization, yielding tight performance bounds.</p>

    <p><strong>Postdoc work (2024–now).</strong> In my postdoc, I'm working on repeated games arising from the competition among agents in proportional allocation auctions. The objective is to capture the system's evolution based on agents' selfish behavior. To answer these questions, our study integrates concepts from game theory and multi-agent online convex optimization.</p>

    <h3>Streaming Algorithms: Count-Min Sketch with Conservative Updates</h3>
    <p><strong>Context.</strong> Online counting of items' occurrences in a data stream with a high arrival rate is crucial to extract key features of the stream, e.g., detection of top k items.
        Because the stream may contain billions of distinct items, streaming algorithms are forced to operate with a memory sublinear in the number of distinct items.
        Under such tight space constraints, streaming tasks like detecting the top-k items inevitably incur some errors.
        The Count-Min Sketch (CMS) is a widely adopted solution: it keeps a compact array of counters that supports approximate frequency queries for each item on the stream, offering a trade-off between memory and accuracy.
        A refinement, CMS with Conservative Update (CMS-CU), boosts accuracy but unlike the vanilla CMS whose memory–accuracy trade-off is well understood, theoretical guarantees for CMS-CU are still missing.</p>
    <p><strong>Contributions.</strong> We develop tractable analytic frameworks that bound the frequency estimation error of CMS-CU in two complementary regimes:
        (i) frequent items under stationary stochastic streams <a href="https://ieeexplore.ieee.org/document/9798146">[C1</a>, <a href="https://doi.org/10.1016/j.comnet.2022.109315">J1]</a> and
        (ii) infrequent items under arbitrary streams <a href="https://dl.acm.org/doi/10.1145/3727135">[C5</a>, <a href="https://dl.acm.org/doi/10.1145/3726854.3727329">J3]</a>).
        For both settings, the main obstacle is dealing with processes similar to balls and bins with power of d,
        except that, on ties, all minimal bins among selected ones are incremented instead of a tie-breaking rule where a single bin is incremented.
        In [C1, J1], we show that the analysis boils down to studying a constrained variant of the aforementioned process:
        a random hypergraph is drawn once, its vertices are the counters, each hyperedge encodes an admissible d-tuple,
        and each hyperedge is selected at every step according to a fixed, non-uniform probability distribution modeling the heterogeneity of the items in the stream.
        On the other hand, in [C5, J3], all hyperedges are admissible and have homogeneous probabilities of being selected. </p>

    <h3>No-Regret Caching and Similarity Caching</h3>
    <p><strong>Context (No-Regret Caching).</strong> Caches are small memories that speed up data retrieval. Caching policies strategically select cache content based on previous requests to maximize a utility function,
        such as the number of requests answered directly by the cache.
        Worst-case analysis of caching algorithms evaluates their performance against an optimal policy with knowledge of future requests.
        One way to do that is to consider the regret metric, which is the difference between the utility of a static optimum and that of the algorithm.
        Numerous caching policies that leverage online convex optimization and exhibit vanishing regret have been proposed.
        Nevertheless, these algorithms are computationally expensive and necessitate complete knowledge of prior requests, which may not be feasible in practical scenarios like caching at a cellular base station. </p>

    <p><strong>Contributions.</strong> In <a href="https://arxiv.org/abs/2503.02758">[J4</a>, <a href="https://arxiv.org/abs/2309.02055">C3]</a>), we show that a Follow-The-Perturbed Leader adaptation achieves near-optimal regret bounds in a partial-observability setting with O(1) update time.</p>


      <p><strong>Context (Similarity Caching).</strong> In this problem, items are represented as points within a metric space, enabling the quantification of similarity between any two items through their distance.
          Similarity caching extends the traditional caching problem by allowing requests for items to be approximately satisfied with similar cached items.
          The objective is to optimize a utility function that considers how similar the requested item is to the cached item used for the response.  </p>

    <p><strong>Contributions.</strong> In <a href="https://doi.org/10.1016/j.comnet.2024.110206">[J2</a>, <a href="https://ieeexplore.ieee.org/document/10000890">C2]</a>, we propose an analytical framework for computing the hit ratio, i.e., the fraction of requests satisfied directly by the cache,
        of an adaptation of the well-known Least-Recently-Used (LRU) policy to similarity caching under stationary requests.
        This computation is challenging as it involves examining a Markov process with exponentially growing state space.
        To make the problem tractable, we introduced a surrogate similarity caching policy with independent caching decisions
        and identified conditions where this fictitious policy closely approximates the LRU-based similarity caching variant. </p>

    <h3>Repeated Proportional Allocation Auctions Games</h3>
    <p><strong>Context.</strong> Decentralized resource allocation in large-scale systems is a fundamental problem that has been extensively studied in network economics.
        The Kelly, also called proportional-auction, is a simple and efficient mechanism in which each agent receives a share of the resource proportional to its bid.
        The resulting game has been shown to achieve near-optimal social welfare at its Nash equilibrium.
        These guarantees, however, assume that agents can actually play this equilibrium, which in turn requires every player to know the utilities of all others.
        A more realistic setting assumes that each agent knows only its own utility, yet can adapt its bid over repeated, synchronous rounds using limited feedback, such as the aggregate bid of all players.</p>
    <p><strong>Contributions.</strong> In <a href="#publications-simple">[C4]</a>, we study the repeated game induced by the Kelly mechanism. We prove that when all agents update
        their bids via a dual-averaging no-regret algorithm and their utilities grow logarithmically with the resource they receive, the system converges to the Nash equilibrium of the one-shot game.</p>

  </div>
</body>
</html>
